<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<!-- Generated by HsColour, http://code.haskell.org/~malcolm/hscolour/ -->
<title>src/DeepBanana/Layer/CUDA.hs</title>
<link type='text/css' rel='stylesheet' href='hscolour.css' />
</head>
<body>
<pre><a name="line-1"></a><span class='hs-comment'>{-# LANGUAGE TemplateHaskell #-}</span>
<a name="line-2"></a><span class='hs-keyword'>module</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span> <span class='hs-layout'>(</span>
<a name="line-3"></a>    <span class='hs-keyword'>module</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>Monad</span>
<a name="line-4"></a>  <span class='hs-layout'>,</span> <span class='hs-keyword'>module</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>CuDNN</span>
<a name="line-5"></a>  <span class='hs-layout'>,</span> <span class='hs-keyword'>module</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>Cublas</span>
<a name="line-6"></a>  <span class='hs-layout'>,</span> <span class='hs-keyword'>module</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>CuRAND</span>
<a name="line-7"></a>  <span class='hs-layout'>,</span> <span class='hs-keyword'>module</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>Numeric</span>
<a name="line-8"></a>  <span class='hs-layout'>,</span> <span class='hs-varid'>softmax</span>
<a name="line-9"></a>  <span class='hs-layout'>,</span> <span class='hs-varid'>lreshape</span>
<a name="line-10"></a>  <span class='hs-layout'>,</span> <span class='hs-varid'>toScalar</span>
<a name="line-11"></a>  <span class='hs-layout'>,</span> <span class='hs-varid'>mlrCost</span>
<a name="line-12"></a>  <span class='hs-layout'>)</span> <span class='hs-keyword'>where</span>
<a name="line-13"></a>
<a name="line-14"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>Data</span><span class='hs-varop'>.</span><span class='hs-conid'>Proxy</span>
<a name="line-15"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>GHC</span><span class='hs-varop'>.</span><span class='hs-conid'>TypeLits</span>
<a name="line-16"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>Prelude</span> <span class='hs-varid'>hiding</span> <span class='hs-layout'>(</span><span class='hs-varid'>id</span><span class='hs-layout'>,</span> <span class='hs-layout'>(</span><span class='hs-varop'>.</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span>
<a name="line-17"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span>
<a name="line-18"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>Monad</span>
<a name="line-19"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>CuDNN</span>
<a name="line-20"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>Cublas</span>
<a name="line-21"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>CuRAND</span>
<a name="line-22"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Layer</span><span class='hs-varop'>.</span><span class='hs-conid'>CUDA</span><span class='hs-varop'>.</span><span class='hs-conid'>Numeric</span>
<a name="line-23"></a><span class='hs-keyword'>import</span> <span class='hs-conid'>DeepBanana</span><span class='hs-varop'>.</span><span class='hs-conid'>Tensor</span>
<a name="line-24"></a>
<a name="line-25"></a><a name="softmax"></a><span class='hs-comment'>-- "Naive" CUDA implementation of softmax, as workaround to bug in current</span>
<a name="line-26"></a><span class='hs-comment'>-- CuDNN-based softmax.</span>
<a name="line-27"></a><span class='hs-definition'>softmax</span> <span class='hs-keyglyph'>::</span> <span class='hs-layout'>(</span><span class='hs-conid'>TensorScalar</span> <span class='hs-varid'>a</span><span class='hs-layout'>,</span> <span class='hs-conid'>KnownNat</span> <span class='hs-varid'>n</span><span class='hs-layout'>,</span> <span class='hs-conid'>KnownNat</span> <span class='hs-varid'>m</span><span class='hs-layout'>)</span>
<a name="line-28"></a>        <span class='hs-keyglyph'>=&gt;</span> <span class='hs-conid'>Layer</span> <span class='hs-conid'>CUDA</span> <span class='hs-varid'>a</span> <span class='hs-chr'>'</span><span class='hs-conid'>[]</span> <span class='hs-layout'>(</span><span class='hs-conid'>Tensor</span> <span class='hs-keyglyph'>[</span><span class='hs-varid'>n</span><span class='hs-layout'>,</span><span class='hs-varid'>m</span><span class='hs-keyglyph'>]</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-conid'>Tensor</span> <span class='hs-keyglyph'>[</span><span class='hs-varid'>n</span><span class='hs-layout'>,</span><span class='hs-varid'>m</span><span class='hs-keyglyph'>]</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span>
<a name="line-29"></a><span class='hs-definition'>softmax</span> <span class='hs-keyglyph'>=</span>
<a name="line-30"></a>  <span class='hs-varid'>lexp</span>
<a name="line-31"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>id</span> <span class='hs-varop'>&amp;&amp;&amp;</span> <span class='hs-layout'>(</span><span class='hs-varid'>sumRows</span> <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>lreshape</span> <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>replicateAsCols</span> <span class='hs-layout'>(</span><span class='hs-conid'>Proxy</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>Proxy</span> <span class='hs-varid'>m</span><span class='hs-layout'>)</span> <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>inv</span><span class='hs-layout'>)</span>
<a name="line-32"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>multiply</span>
<a name="line-33"></a>
<a name="line-34"></a><a name="lreshape"></a><span class='hs-definition'>lreshape</span> <span class='hs-keyglyph'>::</span> <span class='hs-layout'>(</span><span class='hs-conid'>TensorScalar</span> <span class='hs-varid'>a</span><span class='hs-layout'>,</span> <span class='hs-conid'>Shape</span> <span class='hs-varid'>s1</span><span class='hs-layout'>,</span> <span class='hs-conid'>Shape</span> <span class='hs-varid'>s2</span><span class='hs-layout'>,</span> <span class='hs-conid'>Size</span> <span class='hs-varid'>s1</span> <span class='hs-keyglyph'>~</span> <span class='hs-conid'>Size</span> <span class='hs-varid'>s2</span><span class='hs-layout'>)</span>
<a name="line-35"></a>         <span class='hs-keyglyph'>=&gt;</span> <span class='hs-conid'>Layer</span> <span class='hs-conid'>CUDA</span> <span class='hs-varid'>a</span> <span class='hs-chr'>'</span><span class='hs-conid'>[]</span> <span class='hs-layout'>(</span><span class='hs-conid'>Tensor</span> <span class='hs-varid'>s1</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-conid'>Tensor</span> <span class='hs-varid'>s2</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span>
<a name="line-36"></a><span class='hs-definition'>lreshape</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>combinePasses'</span> <span class='hs-varid'>fwdmul</span> <span class='hs-varid'>bwdmul</span>
<a name="line-37"></a>  <span class='hs-keyword'>where</span> <span class='hs-varid'>fwdmul</span> <span class='hs-varid'>x</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>return</span> <span class='hs-varop'>$</span> <span class='hs-varid'>reshape</span> <span class='hs-varid'>x</span>
<a name="line-38"></a>        <span class='hs-varid'>bwdmul</span> <span class='hs-keyword'>_</span> <span class='hs-keyword'>_</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>return</span> <span class='hs-varop'>$</span> <span class='hs-keyglyph'>\</span><span class='hs-varid'>upgrad</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-varid'>reshape</span> <span class='hs-varid'>upgrad</span>
<a name="line-39"></a>
<a name="line-40"></a><a name="toScalar"></a><span class='hs-definition'>toScalar</span> <span class='hs-keyglyph'>::</span> <span class='hs-layout'>(</span><span class='hs-conid'>TensorScalar</span> <span class='hs-varid'>a</span><span class='hs-layout'>,</span> <span class='hs-conid'>Shape</span> <span class='hs-varid'>s</span><span class='hs-layout'>,</span> <span class='hs-conid'>Size</span> <span class='hs-varid'>s</span> <span class='hs-keyglyph'>~</span> <span class='hs-num'>1</span><span class='hs-layout'>)</span>
<a name="line-41"></a>         <span class='hs-keyglyph'>=&gt;</span> <span class='hs-conid'>Layer</span> <span class='hs-conid'>CUDA</span> <span class='hs-varid'>a</span> <span class='hs-chr'>'</span><span class='hs-conid'>[]</span> <span class='hs-layout'>(</span><span class='hs-conid'>Tensor</span> <span class='hs-varid'>s</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span> <span class='hs-varid'>a</span>
<a name="line-42"></a><span class='hs-definition'>toScalar</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>noWeights</span> <span class='hs-varop'>$</span> <span class='hs-varid'>fwdBwdToScalar</span>
<a name="line-43"></a>  <span class='hs-keyword'>where</span> <span class='hs-varid'>fwdBwdToScalar</span> <span class='hs-varid'>x</span> <span class='hs-keyglyph'>=</span> <span class='hs-keyword'>do</span>
<a name="line-44"></a>          <span class='hs-varid'>return</span> <span class='hs-layout'>(</span><span class='hs-varid'>head</span> <span class='hs-varop'>$</span> <span class='hs-varid'>toList</span> <span class='hs-varid'>x</span><span class='hs-layout'>,</span> <span class='hs-keyglyph'>\</span><span class='hs-varid'>upgrad</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-varid'>fromList</span> <span class='hs-keyglyph'>[</span><span class='hs-varid'>upgrad</span><span class='hs-keyglyph'>]</span><span class='hs-layout'>)</span>
<a name="line-45"></a>
<a name="line-46"></a><a name="mlrCost"></a><span class='hs-definition'>mlrCost</span> <span class='hs-keyglyph'>::</span> <span class='hs-keyword'>forall</span> <span class='hs-varid'>a</span> <span class='hs-varid'>n</span> <span class='hs-varid'>m</span> <span class='hs-varop'>.</span> <span class='hs-layout'>(</span><span class='hs-conid'>TensorScalar</span> <span class='hs-varid'>a</span><span class='hs-layout'>,</span> <span class='hs-conid'>KnownNat</span> <span class='hs-varid'>n</span><span class='hs-layout'>,</span> <span class='hs-conid'>KnownNat</span> <span class='hs-varid'>m</span><span class='hs-layout'>)</span>
<a name="line-47"></a>        <span class='hs-keyglyph'>=&gt;</span> <span class='hs-conid'>Layer</span> <span class='hs-conid'>CUDA</span> <span class='hs-varid'>a</span> <span class='hs-chr'>'</span><span class='hs-conid'>[]</span> <span class='hs-layout'>(</span><span class='hs-conid'>Tensor</span> <span class='hs-keyglyph'>[</span><span class='hs-varid'>n</span><span class='hs-layout'>,</span><span class='hs-varid'>m</span><span class='hs-keyglyph'>]</span> <span class='hs-varid'>a</span><span class='hs-layout'>,</span> <span class='hs-conid'>Tensor</span> <span class='hs-keyglyph'>[</span><span class='hs-varid'>n</span><span class='hs-layout'>,</span><span class='hs-varid'>m</span><span class='hs-keyglyph'>]</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span> <span class='hs-varid'>a</span>
<a name="line-48"></a><span class='hs-definition'>mlrCost</span> <span class='hs-keyglyph'>=</span>
<a name="line-49"></a>  <span class='hs-varid'>id</span> <span class='hs-varop'>***</span> <span class='hs-layout'>(</span><span class='hs-varid'>add</span> <span class='hs-varop'>-&lt;</span> <span class='hs-num'>10E-5</span> <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>softmax</span><span class='hs-layout'>)</span>
<a name="line-50"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>multiply</span>
<a name="line-51"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>sumRows</span>
<a name="line-52"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>add</span> <span class='hs-varop'>-&lt;</span> <span class='hs-num'>10E-5</span>
<a name="line-53"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>llog</span>
<a name="line-54"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>sumCols</span>
<a name="line-55"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>scale</span> <span class='hs-varop'>-&lt;</span> <span class='hs-layout'>(</span><span class='hs-comment'>-</span><span class='hs-num'>1</span> <span class='hs-varop'>/</span> <span class='hs-varid'>fromIntegral</span> <span class='hs-layout'>(</span><span class='hs-varid'>natVal</span> <span class='hs-layout'>(</span><span class='hs-conid'>Proxy</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>Proxy</span> <span class='hs-varid'>n</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span>
<a name="line-56"></a>  <span class='hs-varop'>&gt;+&gt;</span> <span class='hs-varid'>toScalar</span>
</pre></body>
</html>
